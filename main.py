from agents.scraper_agent import ScraperAgent
from agents.summarizer_agent import SummarizerAgent
from agents.classifier_agent import ClassifierAgent
from agents.landing_page_agent import LandingPageWatcherAgent
from agents.twitter_agent_selenium import find_twitter_username_from_website, TwitterSeleniumScraper
from agents.youtube_agent import YouTubeAgent, find_youtube_channel_from_website
from agents.notion_agent import NotionAgent
from textwrap import dedent

import json
from pathlib import Path


def run_all_agents():
    print("\nğŸš€ Starting TrackTect AI Agent Workflow...\n")

    # ğŸ”¹ Ask user to input one or more URLs (comma-separated)
    user_input = input("ğŸŒ Enter one or more competitor website URLs (comma-separated):\n").strip()
    input_urls = [url.strip() for url in user_input.split(",") if url.strip()]

    # 1ï¸âƒ£ Scrape Website Content
    print("\nğŸ” Running Scraper Agent...")
    scraper = ScraperAgent()
    scraper.urls = input_urls
    scraped_data = scraper.run()

    # 2ï¸âƒ£ Summarize
    print("\nğŸ“ Running Summarizer Agent...")
    summarizer = SummarizerAgent()
    summaries = {url: summarizer.summarize(text, url) for url, text in scraped_data.items()}

    # 3ï¸âƒ£ Classify
    print("\nğŸ“Š Running Classifier Agent...")
    classifier = ClassifierAgent()
    classified_data = {}
    for url, summary in summaries.items():
        print(f"\nğŸ”— {url}")
        print(f"Summary:\n{summary}")
        categories = classifier.classify(summary, url)
        for item in categories:
            print(f"- [{item['category']}] {item['text']}")
        domain = url.split("//")[-1].split("/")[0].replace(".", "_")
        classified_data[domain] = categories

    # ğŸ’¾ Save classification output
    output_path = Path("output/classified_results.json")
    output_path.parent.mkdir(exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(classified_data, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… Classification results saved to: {output_path.resolve()}")

    # 4ï¸âƒ£ Check Landing Page Messaging Changes
    print("\nğŸ” Running Landing Page Change Detector...")
    landing_checker = LandingPageWatcherAgent()
    landing_checker.run(input_urls)

    # 5ï¸âƒ£ Twitter Handle Detection + Tweet Scraping
    print("\nğŸ¦ Running Twitter Agent...")
    for url in input_urls:
        twitter_handle = find_twitter_username_from_website(url)
        if twitter_handle:
            print(f"\nğŸ”— From {url} â†’ Found Twitter: @{twitter_handle}")
            twitter_scraper = TwitterSeleniumScraper(twitter_handle, max_tweets=5)
            tweets = twitter_scraper.scrape()
            for i, tweet in enumerate(tweets):
                print(f"\nğŸ“ Tweet {i + 1}:\n{tweet}\n{'-' * 40}")
        else:
            print(f"âŒ No Twitter handle found on {url}")
            fallback = input("ğŸ” Do you want to manually enter a Twitter handle? (yes/no): ").strip().lower()
            if fallback == "yes":
                manual_handle = input("âœï¸ Enter handle (without @): ").strip()
                twitter_scraper = TwitterSeleniumScraper(manual_handle, max_tweets=5)
                tweets = twitter_scraper.scrape()
                for i, tweet in enumerate(tweets):
                    print(f"\nğŸ“ Tweet {i + 1}:\n{tweet}\n{'-' * 40}")

    # 6ï¸âƒ£ YouTube Channel Detection + Video Scraping
    print("\nğŸ“º Running YouTube Agent...")

    for url in input_urls:
        yt_channel_url = find_youtube_channel_from_website(url)

        if yt_channel_url:
            print(f"\nğŸ”— From {url} â†’ Found YouTube: {yt_channel_url}")
        else:
            print(f"âŒ No YouTube channel found on {url}")
            fallback = input("ğŸ” Do you want to manually enter the YouTube channel URL? (yes/no): ").strip().lower()
            if fallback == "yes":
                yt_channel_url = input("âœï¸ Enter the YouTube channel URL: ").strip()

        if yt_channel_url:
            yt_scraper = YouTubeAgent(yt_channel_url, max_videos=5)
            videos = yt_scraper.scrape()

            for i, video in enumerate(videos):
                print(f"\nğŸ¬ Video {i + 1}: {video['title']}")
                print(f"ğŸ”— {video['url']}")
                print(f"ğŸ“ Description: {video['description'][:200]}...")
                print("ğŸ’¬ Comments:")
                for c in video['comments']:
                    print(f" - {c}")
                print("-" * 40)


    print("\nğŸ§  Pushing insights to Notion...")
    notion = NotionAgent()
    for domain, items in classified_data.items():

        summary_lines = [f"{domain.replace('_', '.')} â€” Latest classified updates:"]
        for item in items:
            summary_lines.append(f"- [{item['category']}] {item['text']}")
        formatted_summary = "\n".join(summary_lines)
        notion.append_update(title=domain, content=formatted_summary)


    print("âœ… Updates pushed to Notion!")

    print("\nğŸ‰ All agents executed successfully!\n")




if __name__ == "__main__":
    run_all_agents()
